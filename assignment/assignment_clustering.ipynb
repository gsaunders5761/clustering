{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cfc23963-2cc5-4f1d-8278-fb1b2026afc5",
      "metadata": {
        "id": "cfc23963-2cc5-4f1d-8278-fb1b2026afc5"
      },
      "source": [
        "## Assignment: $k$ Means Clustering\n",
        "\n",
        "### `! git clone https://www.github.com/ds4e/clustering`\n",
        "\n",
        "### **Do Q1 and one other question.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26523935-9e8f-4377-920c-6f65605c0e31",
      "metadata": {
        "id": "26523935-9e8f-4377-920c-6f65605c0e31"
      },
      "source": [
        "**Q1.** This is a question about clustering. We want to investigate how adjusting the \"noisiness\" of the data impacts the quality of the algorithm and the difficulty of picking $k$.\n",
        "\n",
        "1. Run the code below, which creates four datasets: `df0_125`, `df0_25`, `df0_5`, `df1_0`, and `df2_0`. Each data set is created by increasing the amount of `noise` (standard deviation) around the cluster centers, from `0.125` to `0.25` to `0.5` to `1.0` to `2.0`.\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3f085a16",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "da7c6814",
      "metadata": {},
      "outputs": [],
      "source": [
        "def createData(noise,N=50):\n",
        "    np.random.seed(100) # Set the seed for replicability\n",
        "    # Generate (x1,x2,g) triples:\n",
        "    X1 = np.array([np.random.normal(1,noise,N),np.random.normal(1,noise,N)])\n",
        "    X2 = np.array([np.random.normal(3,noise,N),np.random.normal(2,noise,N)])\n",
        "    X3 = np.array([np.random.normal(5,noise,N),np.random.normal(3,noise,N)])\n",
        "    # Concatenate into one data frame\n",
        "    gdf1 = pd.DataFrame({'x1':X1[0,:],'x2':X1[1,:],'group':'a'})\n",
        "    gdf2 = pd.DataFrame({'x1':X2[0,:],'x2':X2[1,:],'group':'b'})\n",
        "    gdf3 = pd.DataFrame({'x1':X3[0,:],'x2':X3[1,:],'group':'c'})\n",
        "    df = pd.concat([gdf1,gdf2,gdf3],axis=0)\n",
        "    return df\n",
        "\n",
        "df0_125 = createData(0.125)\n",
        "df0_25 = createData(0.25)\n",
        "df0_5 = createData(0.5)\n",
        "df1_0 = createData(1.0)\n",
        "df2_0 = createData(2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f799ed2",
      "metadata": {},
      "source": [
        "\n",
        "2. Make scatterplots of the $(X1,X2)$ points by group for each of the datasets. As the `noise` goes up from 0.125 to 2.0, what happens to the visual distinctness of the clusters?\n",
        "3. Create a scree plot for each of the datasets. Describe how the level of `noise` affects the scree plot (particularly the presence of a clear \"elbow\") and your ability to definitively select a $k$. (Pay attention to the vertical axis across plots, or put all the scree curves on a single canvas.)\n",
        "4. Explain the intuition of the elbow, using this numerical simulation as an example."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8013bcba",
      "metadata": {},
      "source": [
        "**Q2.** This question is a case study on clustering.\n",
        "\n",
        "1. Load the `2022 election cycle fundraising.csv` file in the `./data` folder. This has campaign finance data for the 2022 election for House and Senate candidates. We're going to focus on the total amount they raised, `Raised`, the total amount they spent, `Spent`, their available `Cash on Hand`, and their `Debts`. The variables denominated in dollars are messy and require cleaning. How do you handle it?\n",
        "2. Max-min normalize `Raised` and `Spent`. Use a scree plot to determine the optimal number of clusters for the $k$ means clustering algorithm. Make a scatter plot of `Raised` against `Spent` and hue the dots by their cluster membership. What do you see? Which politicians comprise the smallest two clusters? If necessary, look up some of these races to see how close they were.\n",
        "3. Repeat part 2, but for `Cash on Hand` and `Debts`. Compare your results with part 2. Why might this be? If necessary, look up some of these races to see how close they were.\n",
        "4. Use $k$ means clustering with all four numeric variables. How do your results compare to the previous two parts?\n",
        "5. Did the $k$-MC algorithm find useful patterns for you in analyzing the election?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc78796",
      "metadata": {},
      "source": [
        "**Q3.** This question is a case study on clustering.\n",
        "\n",
        "1. Load the `SIPRI Military Expenditure Database.csv` file in the `./data` folder. This has data about military spending by country. Filter the rows to select only the year 2020, and drop all rows with missing values. I ended up with 148 countries. Is any further cleaning of the variables required?\n",
        "2. Max-min normalize `Spending (2020 USD)` and `Spending per Capita`. Use a scree plot to determine the optimal number of clusters for the $k$ means clustering algorithm. Make a scatter plot of `Spending (2020 USD)` and `Spending per Capita`, and hue the dots by their cluster membership. Compute a describe table conditional on cluster membership (i.e. `.groupby(cluster).describe()`). What do you see? Where is the United States? Do you notice any patterns in the cluster membership?\n",
        "3. Repeat part 2 for `Percent of Government Spending` and `Percent of GDP`. How do your results compare to part 2?\n",
        "4. Use $k$ means clustering with all four numeric variables: `Spending (2020 USD)`, `Spending per Capita`, `Percent of Government Spending`, and `Percent of GDP`. How do your results compare to the previous two parts? \n",
        "5. Did the $k$-MC algorithm find any useful patterns for you in analyzing the election?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85dae156-b49b-4f43-bc42-ad1589132891",
      "metadata": {
        "id": "85dae156-b49b-4f43-bc42-ad1589132891"
      },
      "source": [
        "**Q4.** This question is a case study for $k$ means clustering.\n",
        "\n",
        "1. Load the `airbnb_hw.csv` data. Clean `Price` along with `Beds`, `Number of Reviews`, and `Review Scores Rating`.\n",
        "2. Maxmin normalize the data and remove any `nan`'s (`KMeans` from `sklearn` doesn't accept `nan` input).\n",
        "3. Use `sklearn`'s `KMeans` module to cluster the data by `Beds`, `Number of Reviews`, and `Review Scores Rating` for `k=6`.\n",
        "4. Use `seaborn`'s `.pairplot()` to make a grid of scatterplots that show how the clustering is carried out in multiple dimensions.\n",
        "5. Use `.groupby` and `.describe` to compute the average price for each cluster. Which clusters have the highest rental prices?\n",
        "6. Use a scree plot to pick the number of clusters and repeat steps 4 and 5."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "870041b4-204b-4bc4-bf8a-3fe2f8734a58",
      "metadata": {
        "id": "870041b4-204b-4bc4-bf8a-3fe2f8734a58"
      },
      "source": [
        "**Q5.** We looked at computer vision with $k$NN in a previous question. Can $k$ means clustering correctly group digits, even if we don't know which symbols are which?\n",
        "\n",
        "1. To load the data, run the following code in a chunk:\n",
        "```\n",
        "from keras.datasets import mnist\n",
        "df = mnist.load_data('minst.db')\n",
        "train,test = df\n",
        "X_train, y_train = train\n",
        "X_test, y_test = test\n",
        "```\n",
        "The `y_test` and `y_train` vectors, for each index `i`, tell you want number is written in the corresponding index in `X_train[i]` and `X_test[i]`. The value of `X_train[i]` and `X_test[i]`, however, is a 28$\\times$28 array whose entries contain values between 0 and 256. Each element of the matrix is essentially a \"pixel\" and the matrix encodes a representation of a number. To visualize this, run the following code to see the first ten numbers:\n",
        "```\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.set_printoptions(edgeitems=30, linewidth=100000)\n",
        "for i in range(5):\n",
        "    print(y_test[i],'\\n') # Print the label\n",
        "    print(X_test[i],'\\n') # Print the matrix of values\n",
        "    plt.contourf(np.rot90(X_test[i].transpose())) # Make a contour plot of the matrix values\n",
        "    plt.show()\n",
        "```\n",
        "OK, those are the data: Labels attached to handwritten digits encoded as a matrix.\n",
        "\n",
        "2. What is the shape of `X_train` and `X_test`? What is the shape of `X_train[i]` and `X_test[i]` for each index `i`? What is the shape of `y_train` and `y_test`?\n",
        "3. Use Numpy's `.reshape()` method to covert the training and testing data from a matrix into an vector of features. So, `X_test[index].reshape((1,784))` will convert the $index$-th element of `X_test` into a $28\\times 28=784$-length row vector of values, rather than a matrix. Turn `X_train` into an $N \\times 784$ matrix $X$ that is suitable for scikit-learn's kNN classifier where $N$ is the number of observations and $784=28*28$ (you could use, for example, a `for` loop).\n",
        "4. Use $k$ means clustering on the reshaped `X_test` data with `k=10`.  \n",
        "5. Cross tabulate the cluster assignments with the true labels for the test set values. How good is the correspondence? What proportion of digits are clustered correctly? Which digits are the hardest to distinguish from one another? Can $k$MC recover the latent digits 0 to 9, without even knowing what those digits were?\n",
        "6. If you use a scree plot to determine the number of clusters $k$, does it pick 10 (the true number of digits), or not? If it fails to pick $k=10$, which digits does it tend to combine into the same classification?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
